---
title: ElasticSearch 세미나
date: 2022-09-07 14:14:00 +0900
category: Edu
draft: false
---

# 2022-09-07 ElasticSearch 세미나

# 1교시

## 01.History

- 데이터 넣을때 elk 쓸때 로그스태시 사용
- Kibana, elastic, logstash
  - elk
- 2011년에 회사 창업....
- 아파치 라이센스로 제공을 하다가
  - 오픈소스가 비지니스를 해서 성공하는 경우가 많지 않음
  - 처음 대형회사에서 개발하다가 오픈소스로 공개해서 하는 경우 
    - 대기업이 시작을 해서 망하는 경우 없었음
  - 오픈소스가 인기가 많아져서 성공하는 경우는 적음
- 배포하다가, 유료로 제공 진행
- 그 후에 클라우드 서비스를 많이함
- 상용을 위한 보안, 알림, 스택자체를 모니터링하는 기능 상용으로 만드러서 유저들에게 제공함
  - 보안(로그인, 통신 암호화)- 실드
  - 알람 (워쳐)
  - 모니터링 (마블)
- 이전에는 각각 개발을 하다보니 버전이 들쑥날쑥함

- elk는 5버전 이후에 얼라이닝시작
- xpack, watcher이런거 기존에 api가 남아있는데
  - 이름이 이렇게 됬는데 디지니가 이름 쓰지말라고해서 못쓰고 재미 없는 이름 쓰는중
- 5.0 ~ 6.3 버전으로 가면서 라이브러리를 바꾸게됨
- 베이직 기능이 있었지만 잘 사용하지 않았음
  - 그래서상용 베이직 같이 내려받게 해놓음
- 라이센스의 경우 
  - 별도 설치없이 라이센스적용시 enable할 수 있게 해줌
- 7.x~ 
  - 기능이 많아져서 하나로 묶어짐

## 02.elasticsearch

- 클러스터로 동작하는 
- 스케일 아웃이 잘되어 있음
  - 처음 만들어졌을때 부터 염두 되어서 만들어짐

- 아파치루신을 사용함
  - 이를 통해 검색 엔진을 만듦
  - elasticsearch
    - 상용으로 더 많이 씀
  - Solr
    - 이 경우 지원하는 회사 적음

- 더그 커팅
  - 하둡이랑 등등 루신등을 만든 사람
  - 루씬 만들때 왜 자바로 만들었나?
    - 편의성, 이런 쉬워서?
    - 자바를 공부하려고 만든것이 루씬이라고함
  - Elastic에서 루씬을 고쳐서도 많이씀
    - 소스 레벨을 건들어야하는 경우
    - 제품에 내재화 할때, 그때 마다 반영힘들고 어려울 수 있음
    - elastic의 경우 루씬 커미터 20여명정도 있었음

## 03.클러스터링 과정

- 샤드 단위
- 실행단위 노드라고함
- 데이터 한건 한건 도큐먼트 그것의 합쳐진것 인데스
- 인덱스 설정에 따라서 샤드사용 유무 바뀜
  - 예전에는 5개 였지만 1개만 일단 사용
- 저장소가 꽉차는 경우 노드 실행해주고 네트워크랑 이런것 설정 같이 해놓는 경우 같은 클러스터로 됨
  - 그렇게 해서 용량 나누어 가짐
- 프라이머리 샤드와 리프리카샤드로 나뉘어짐
  - primary는 처음 생성된 샤드
  - 노드가 하나인 경우 리프리카가 생성되지 않음
  - 이때 노드를 따로 만들어놓음
- 노드가 죽으면 살때까지 있다가 안된다 싶으면 다른 노드에 리프리카 데이터 이동해서 무결성 유지함

### 03.1 클러스터

- 클러스터 다르면 데이터 간섭이 일어나지 않음
  - config/elasticsearch.yml
    - 이곳에 클러스터 이름 지정됨
    - 여기서 이름 적용하면 같은 것으로 묶임
      - 13p
- 9300번은 다른 노드들끼리 통신하는 것
- 한 서버에 노드 두개면 알아서 1씩 증가시켜서 포트 선언해서 사용

### 03.2 클러스터 구성

- 16p
- 새 노드 실행시 노드 찾아서 시작할지 합류할지 선언



```
mv elasic node-1
cd node-1

code config/elasticsearch.yml
# cluster.name, node네임 설정

curl localhost:9200
# 제대로 올라왔는지 확인
```

- 두개의 elastic을 실행했을때 클러스터 이름 같고 노드 이름만 다르게 설정

```
curl -XGET "http://localhost:9200/my-index/_doc/1?pretty"{
{
}}
...
```

- 18p

## 04.데이터 구조

- 호스트, 포트, 인덱스 이름, 기능, 도큐먼트 id
  - 메서드를 통해 put,get, delete 사용 가능
- json 도큐먼트 형식을 사용함
  - 구조가 어렵지는 않음
- full text 검색이 가능
  - 위키피디아도 위와 같은 full text로 되어 있음
- rdb는 like할때 하나 하나씩 그것을 찾게 되는데
  - 처음부터 쭉 가게 되는데 full text를 잘 안함 데이터 많아지면 느려지기 때문에
- elk는 각 텍스트를 다 잘라서 역으로 정리
  - inverted index라는 구조로 저장함
  - 책에서 주요 단어 찾아보기 같은 개념이라고 생각하면됨
  - 이런 Term들이 늘어날 수 있지만 찾아가는 시간 많이 늘어나지는 않음
  - 그것의 id 역시 포인터만 늘어날뿐 rdb보다는 확실히 속도가 빠름
- 텍스트 쪼게는것
  - text Analysis
  - pattern replace, mapping 전체에 대해서 단어 처리
- 텍스트 쪼개는것 토크나이저
  - 토크나이저는 하나만 사용가능
  - 공백기준으로 짜르는것하던지 슬래시 기준으로 하는지를 설정할 수 있음
- 위를 잘게 쪼개는것 토큰 필터
  - 한개씩 적용함
- 텀이 어떤 도큐먼트 아이디 가리키는지 저장
- 대문자의 경우 소문자로 바꿔줌
- stop words
  - the, a, an, ar ... 그런것을 필요없는 부분을 다 제외함
- 형태소 분석과정
  - 언어별 영어,들 코드 분석기와
  - 대게 snowball token을 많이 사용
    - jumps나 jumping 을 jump으로 검색할 수 있게 만들어줌
  - quick 을 검색해도 fast동의어도 찾아줌
- 필드 매핑할때 필요하게 만든 필터를 적용할 수 있음

## 05.검색과정

- 32p

## 06.검색 엔진

- 조인은 가능하지만 스크립트 필드로 하지만 트릭임
- elk같은 nosql은 내가 보고 싶은 결과를 자체르 넣어놓고 빨리 찾을 수 있는것 넣어놓은것
  - 그래서 그것을 설계를 맞춰서 사용해야함
- rdb의 조인한 결과를 넣어서 써야함 이것을 불편하다고 생각하면 안됨
- 수정 삭제가 비쌈
  - 책을 비유하면 rdb는 목차떼어내서 제목 만든것
  - elk 는 찾아보기 같은것
    - 5챕터 필요없어서 다 지워야하는경우 rdb의 경우 챕터만 지우면 되지만 
    - elk는 하나한 찾아서 지워야해서 비쌈

## **07.한글 검색** | 35p

- 화이트 스페이스 나누면 안됨
  - 동해물과 백두산이 했을때
    - 동해 라고해서 검색이 안됨

- 동시흥분기점
  - 이거 형태분석하면 음란마귀가 나옴 그래서 두글자 두글자로 나눔
- 커스텀으로 만들어야하는 경우 많음

### 07.1 한글 형태소 분석기

- 커스텀 받아서 써야했는데
- 지금은 버전에 맞춰서 제공하고 있음
  - Nori출시 , 프랑스 사람이 만들었다고 함
  - 한국어 검색이 만들기 힘들때, 창시자에게 한글 검색기 안 만들어주면 안된다 고 했는데 지미라는 애가 만듦
    - 일본형태소의 경우도 만듦
- 은전한닢
  - 메카브 일본어 사전파일 한국어 사전파일로 바꾼것
  - 자기가 참여해서 만들었다고 함
  - 은전한닢 참고하는 레파지토리 있는데 거기에 csv파일로 81만개에 대한 사전에 들어간 내용이 있음
    - 대학, 대학 + 교수... 이라는 것 이라는것이 들어 있음
      - btree형식으로 변경해서 각 점수를 네자리 숫자를 정수형을 short로해서 200mb 14mb로 2주만에 만듦
        - 이를 루신에 반영해서 만듦 

### 07.2 루신인 지라에서 프로젝트 관리함

- 한국어 검색기 - 고도리
  - 고도리라고 이름 되어 있는데
  - 한국어 단어 몇개 해서 편한것 선택한것중 노리가 선택됨
- 노리 : 일본어 김, 행운이라고 하는 뜻임

# 2교시

- elastic 설정 cloud에대해서 설정
  - 최근 일주일 hot
  - 최근 6개월 warm
  - 최근 5년 frozone에 넣음
- 클라우드는 존 이나, 스토리지 설정을 함
  - 스토리지 넒게하면 마스터 설정 별개로 할 수 있게하는것 활성화됨
  - 노드가 많아지면 마스터 노드 만드는것이 좋음
- integrations server
  - 예전에 비트를 이용해서 했는데 이제 atm 서비스, 플릿서버, 메트릭비드 파일 비트
    - 그것을 관리하믄 중간 미들웨어 필요 이것이 integrations임
      - 플릿이 있어야 다른 원격으로 관리 용이함
- enterprise Search

## 01.매핑 및 필드 | 40p

- 매핑
  - 스키마를 매핑이라고함
  - 인덱스 만들때 설정해야하는데 도큐먼트만해도 자동으로 생성됨
- 각 타입의 경우 거의 자동으로 해서 알아서 넣어줌
  - 텍스트 경우
    - 처음 키워드 바꿔도됨
    - 타입의 경우 키워드 타입으로 해야함
- 검색시 _search?
  - match 쿼리 사용해서 검색 
- 키워드는 제대로 다 입력해야지 검색이 됨

## 02.키워드 저장이유

- 집계때문에 
  - 집계를 그냥했을때, 통으로 집계되는게 아니라 잘려서 집계가 되었음
  - 아이들 검색시
    - 서태지 아이드, 아이들,.. 검색 되었음
- 각 필드가 어떻게 사용될지 고민하고 사용하면 좋음
- 집계를 할때 쓰이는 경우가 많은 경우
  - category
    - 그럴때 키워드 타입으로 저장하고 검색시 ui 레벨에서 드롭 리스트를 이용해서 컨트롤 하게해서
      - 사용자들이 오타낼 일 없이 사용할 수 있게 할 수 있음
- author
  - ....
- 한국어, 일본어, 중국어 이렇게 있을때, 각각의 언어에 맞게 검색을 하기 위해서 사용

## 03.집계 | 43p

- search 와 같이 쓸 수 있음
- 44p
  - authors임의 지정
  - 필드구분까지 
    - 결과는 제일 처음 나오고
    - 이를 bucket으로 해서 몇건이 나타나는지 나오게됨

- bucket, metric, pipeline
  - metric, 사칙, 최대, 최소
  - piplne. 변위 미분값 등등

-  숫자는 range agg 사용가능

- 날짜는 게이트레인지?
  - 이것도 bucket으로 할 수 있고, 이거으로 레인지 잡지만 이를 히스토그램이라고 하고 
  - 인터벌을 해서 10으로 설정시 10기준으로 버킷을 생성해줌

- metric agg
  - 평균, 합계등등 계산
  - 쿼리 하나에 agg 여러개 할 수 도 있음

- 키바나에서도 agg해서 결과를 만들어냄

## 04.도큐먼트 색인 과정

- 도큐먼트 색인 과정

  ```
  Put my_index/_doc/100
  # 안쓰면 자동으로 들어감
  ```

  - 도큐먼트 아이디를 통해 샤드에 넣고 라운드로빈을 이용해서 넣음

- 도큐먼트 불러오기

  ```
  GET my_index/_doc/101
  ```

- 검색과정

  ```
  GET my_index/_search{size: 10}
  # 기본이 10, 
  #이를 샤드에 10개씩 가져오게 되어 있고 이를 코디네이터로 전달함
  #루신이 자체로 선언한 도큐먼트 아이디와 내부 점수 리턴함
  #샤드 가 코디네이터까지 오면 코디네이트라고함
  #10개를 가지고 와서 40개 버리고 10개 도큐먼트에게 요청해서 그 과정에서 해당 도큐먼트 내용 샤드들이 코디네이터 가 클라이언트에게 전달 Fetch phase
  ```

## 05.랭킹알고리듬

- 처음에는 TF/IDF 
- 지금은 BM25

- TM
  - 문서 많을 수록 점수 오르고
- IDF
  - 만들수록 점수 낮음
  - 많이 출현한 단어 많을 수록 점수 낮음
- 스파이더맨 검색시, 
  - 도큐먼트에 1개, 도큐먼트 10, 도큐먼트 100 일때 
    - 100이 사용자가 더 선호 하는 것 이때 TM
- spiderman 할때 spider + man 분리 해서 무튼 많을 수록 점수 낮은것 IDF

## 06.검색 랭킹 중요이유

- 사람들 처음 나온값만 봄
  - 1만개이상은 안됨
  - 사용시 from을 쓸수 있는데
  - 그 위치부터 몇개를 가져올 수 있을지 설정
    - 이렇게 하면 페이지 처리를 할 수 있음
    - 페이지 넘어갈 수록 느려진다고 하는데 
    - 999, 10 이면 페이지 99개 가져오고
      - 페이지 해야하는 경우 다른것 피드백줘서 하는게 좋음 비용이 너무 비싸서

- 구글 URL보면 start=10 이런게 있는데 이를 설정하면 60이면 6페이지, 200이면 20페이지 이렇게 되는것
  - 900하면 검색이 안되는데 990억개라는것 거짓말임
  - 검색엔진은 대략위와 같이 동작함
  - 필요없는것 까지 굳이 만드는것 아님

## 07. 루씬 세그먼트

- 1초마다 세그먼트를 만듦
  - 그래서 파일 갯수가 많음
  - 책을 넘지면 1초동안 내용가지고 인벌트인덱스만들어서 저장함
  - 한번 생성된 세그먼트 변하지 않음
    - 이때 도큐먼트 삭제되면
      - 그 도큐먼트는 새로 들어오는 세그먼트에 저장소에 기록..?
      - 그래서 삭제되는거 빼고 검색하는데
      - 많아지면 한번 세그먼트 머지하는데 삭제된것 빼고, 나머지만 병합을 해서 저장
- 세그먼트 머지할때
  - 디스크 아이오라서 느려질 수 있음
  - force merge하면 임의적으로 할 수 있음
- 오래된것 커지고, 새로 생긴것 상대적으로 작음

- lucene segment..
  - changing bits라는것 보면 영상으로 확인 할 수 있음

- 쓰면서 하면안되는것
  - 2년전 100만개 데이터라고 하면 거기서 한번도 쿼리 추가 안했다고 했을때
  - 잘못들어간것때문에 지웠다면 elk는 그 작업을 다시해야함
    - 그래서 로그데이터 쌓기에는 좋음, 바꿀 이유는 없기때문에
  - 삭제시 인덱스 단위로 삭제하는 것이 좋음
- 날짜단위로 하는것이 좋다고 말했지만
  - 안좋은것은 인덱스 개수많아지고 샤드 많아져서 느려지는 현상이 있었음
  - 그래서 5개에서 1개로 샤드 바꿨는데
  - 데이터 프레임 구조를 만들었는데 이 데이터 프레임이 인덱스 관리해줌
    - 날짜 단위가 아니고 20기가마다 새로 만들어주는것 즉, 원하는 사이즈가되면 새로운것 만들고 검색시 같이 검색하는식으로 되게 해주는
    - 데이터 프레임사용하려면 아직은 설정을 따로해야함
      - apm,등등은 데이터 프레임을 사용함

- 처음하는 사람들이 쉽게 하게
  - sample data라는것이 있음
    - other sample data라는 것에 샘플데이터 있는데 이를 이용해서 사용할 수 있게함

# 3교시

- hands on
  - 따로 타임설정안하면
  - 디폴트 최근 15분인데 이유는 데이터 뷰 열때 agg를 돌리는데, 최소한 제한을 안하면 전체 인덱스 뒤질때 부하가 발생시 적게 부하 주려고 적용함
- 이전에 visualize library에서 받아와서 해야했는데 
  - 이제는 대시보드에서 해도됨
- Lense에서도 드래그엔 드롭해서 만들수 있게 되어 있음

- maps.elasitc
  - world_countries
    - 나라가 키워드 타입으로 그 위치에 표기를 할 수 있음
    - Korea provinces,
    - Korea Municiplities
      - 시 군 구 까지 표현됨 
- geoip, 매칭...